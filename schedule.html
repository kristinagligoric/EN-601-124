<div class="container sec" id="schedule" style="margin-top:-20px">
    <br>
    <h2>Content Schedule</h2>
    <p>The current class schedule is below (subject to change).</p>

    <table class="table">
        <colgroup>
            <col style="width:6%">
            <col style="width:18%">
            <col style="width:45%">
        </colgroup>
        <thead>
        <tr class="active">
            <th>Date</th>
            <th>Research questions</th>
            <th>Papers and/or slides</th>
        </tr>
        </thead>
        <tbody>
        <tr>
            <td>#1 - Tue Aug 26</td>
            <td style="background-color: #e5e9ff;">
                Reviewing the foundation:
                <ul>
                    <li>Course overview</li>
                    <li>Plan and expectations</li>
                </ul>
            </td>
            <td>[slides: <a href="files/presentations/0-intro.pptx">pptx</a>, <a
                    href="files/presentations/0-intro.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#2 - Thu Aug 28</td>
            <td style="background-color: #e5e9ff;">
                Reviewing the foundation:
                <ul>
                    <li>Self-supervised learning foundation</li>
                    <li>Masked language modeling</li>
                    <li>Language models and probability</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional
                        Transformers for Language Understanding</a></li>
                    <li>More slides [<a href="files/presentations/1-intro.pptx">pptx</a>, <a
                            href="files/presentations/1-intro.pdf">pdf</a>]
                    </li>
                </ol>
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Sep 1
            </td>
            <td>No class. Homework #1 deadline</td>
            <td>
            </td>
        </tr>
        <tr>
            <td>#3 - Tue Sep 2</td>
            <td style="background-color: #e5e9ff;">
                Reviewing the foundation:
                <ul>
                    <li>Self-supervised learning foundation</li>
                    <li>Autoregressive language modeling</li>
                    <li>Transition to today's models</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
                    <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
                        Models are Unsupervised Multitask Learners</a></li>
                    <li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a> (the
                        GPT3 paper)
                    </li>
                </ol>
                [slides: <a href="files/presentations/2-foundation.pptx">pptx</a>, <a
                    href="files/presentations/2-foundation.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#4 - Thu Sep 4</td>
            <td style="background-color: #f9fff5;">
                Scaling LLMs:
                <ul>
                    <li>How does scale help?</li>
                    <li>Emergence</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2206.07682.pdf">Emergent Abilities of Large Language Models</a>
                    </li>
                </ol>
                [slides: <a href="files/presentations/3-scale.pptx">pptx</a>, <a
                    href="files/presentations/3-scale.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#5 - Tue Sep 9</td>
            <td style="background-color: #ffe6e6;">
                Tokenization / representations:
                <ul>
                    <li>Tokenization</li>
                    <li>Why is tokenization the way it is?</li>
                    <li>What are the issues with subword tokenization?</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/1508.07909.pdf">Byte Pair Encoding and friends</a></li>
                    <li><a href="https://aclanthology.org/2023.acl-long.284.pdf">Tokenization consistency</a></li>
                    <li><a href="https://arxiv.org/pdf/2304.08467.pdf">Tokens considered harmful</a></li>
                    <li><a href="https://arxiv.org/pdf/2406.01763">Spelling issues</a></li>
                </ol>
                [slides: <a href="files/presentations/4-tokenization.pptx">pptx</a>, <a
                    href="files/presentations/4-tokenization.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#6 - Thu Sep 11</td>
            <td style="background-color: #ffeee6;">
                Prompting and in-context learning:
                <ul>
                    <li>Chain-of-thought prompting</li>
                    <li>Variety of prompting techniques</li>
                    <li>What helps in-context learning?</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2201.11903.pdf">Chain of Thought Prompting Elicits Reasoning in
                        Large Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2212.10001.pdf">What is the source of ICL?</a></li>
                    <li><a href="https://arxiv.org/pdf/2305.14739.pdf">In-context Autoregressive Ranking</a></li>
                </ol>
                [slides: <a href="files/presentations/5-prompting.pptx">pptx</a>, <a
                    href="files/presentations/5-prompting.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#7 - Tue Sep 16</td>
            <td style="background-color: #fff7e6;">
                Adapting LLMs via training:
                <ul>
                    <li>Instruction-tuning</li>
                    <li>LoRA</li>
                    <li>Other parameter-efficient ways</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2210.11416.pdf">Scaling Instruction-Finetuned Language Models</a>
                    </li>
                    <li><a href="https://arxiv.org/pdf/2106.09685.pdf">LoRA: Low-Rank Adaptation of Large Language
                        Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2104.08691.pdf">Prefix-tuning</a></li>
                </ol>
                [slides: <a href="files/presentations/6-lora.pptx">pptx</a>, <a
                    href="files/presentations/6-lora.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#8 - Thu Sep 18</td>
            <td style="background-color: #ffffe6;">
                RLHF:
                <ul>
                    <li>Reinforcement Learning from Human Feedback</li>
                    <li>PPO, DPO, and alternatives</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2203.02155.pdf">Training language models to follow instructions
                        with human feedback</a></li>
                    <li><a href="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimization</a></li>
                </ol>
                [slides: <a href="files/presentations/7-rlhf.pptx">pptx</a>, <a
                    href="files/presentations/7-rlhf.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#9 - Tue Sep 23</td>
            <td style="background-color: #e5e9ff;">
                Synthetic data and distillation:
                <ul>
                    <li>Generating synthetic data for training</li>
                    <li>Self-teaching techniques</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2407.13788">Scaling Synthetic Data Creation with 1,000,000,000
                        Personas</a></li>
                    <li><a href="https://arxiv.org/pdf/2312.02119.pdf">Improving Text Embeddings with Large Language
                        Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2308.01825.pdf">Textbooks Are All You Need</a></li>
                </ol>
                [slides: <a href="files/presentations/8-synthetic.pptx">pptx</a>, <a
                    href="files/presentations/8-synthetic.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#10 - Thu Sep 25</td>
            <td style="background-color: #f1ffe6;">
                Multimodality:
                <ul>
                    <li>Vision-Language models</li>
                    <li>Audio and video understanding</li>
                    <li>Cross-modal understanding</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2304.10592.pdf">Flamingo: a Visual Language Model for Few-Shot
                        Learning</a></li>
                    <li><a href="https://arxiv.org/pdf/2312.11805.pdf">Gemini: A Family of Highly Capable Multimodal
                        Models</a></li>
                </ol>
                [slides: <a href="files/presentations/9-multimodality.pptx">pptx</a>, <a
                    href="files/presentations/9-multimodality.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#11 - Tue Sep 30</td>
            <td style="background-color: #e6f4ff;">
                Reasoning and planning:
                <ul>
                    <li>Mathematical reasoning</li>
                    <li>Code generation</li>
                    <li>Planning and search</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2305.14333.pdf">Mathematical Capabilities of ChatGPT</a></li>
                    <li><a href="https://arxiv.org/pdf/2309.15217.pdf">Tree of Thoughts: Deliberate Problem Solving with
                        Large Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2310.08118.pdf">Language Agent Tree Search</a></li>
                </ol>
                [slides: <a href="files/presentations/10-reasoning.pptx">pptx</a>, <a
                    href="files/presentations/10-reasoning.pdf">pdf</a>]
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Oct 2
            </td>
            <td>Paper proposal deadline
            </td>
            <td>
            </td>
        </tr>
        <tr>
            <td>#12 - Thu Oct 2</td>
            <td style="background-color: #e8ffe6;">
                Agents and tool use:
                <ul>
                    <li>LLMs as agents</li>
                    <li>Tool use and function calling</li>
                    <li>Multi-agent systems</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2302.04761.pdf">Toolformer: Language Models Can Teach Themselves
                        to Use Tools</a></li>
                    <li><a href="https://arxiv.org/pdf/2303.17580.pdf">Reflexion: Language Agents with Verbal
                        Reinforcement Learning</a></li>
                    <li><a href="https://arxiv.org/pdf/2308.08155.pdf">AutoGen: Enabling Next-Gen LLM Applications via
                        Multi-Agent Conversation</a></li>
                </ol>
                [slides: <a href="files/presentations/11-agents.pptx">pptx</a>, <a
                    href="files/presentations/11-agents.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#13 - Tue Oct 7</td>
            <td style="background-color: #ffe6f5;">
                Retrieval and knowledge:
                <ul>
                    <li>Retrieval-augmented generation</li>
                    <li>Knowledge editing</li>
                    <li>Long context understanding</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for
                        Knowledge-Intensive NLP Tasks</a></li>
                    <li><a href="https://arxiv.org/pdf/2310.01425.pdf">Editing Large Language Models: Problems, Methods,
                        and Opportunities</a></li>
                </ol>
                [slides: <a href="files/presentations/12-retrieval.pptx">pptx</a>, <a
                    href="files/presentations/12-retrieval.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#14 - Thu Oct 9</td>
            <td style="background-color: #e6f1ff;">
                Evaluation:
                <ul>
                    <li>Benchmarks and their limitations</li>
                    <li>Human evaluation</li>
                    <li>LLM-as-a-judge</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2211.09110.pdf">Holistic Evaluation of Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2306.05685.pdf">Judging LLM-as-a-judge with MT-Bench and Chatbot
                        Arena</a></li>
                </ol>
                [slides: <a href="files/presentations/13-evaluation.pptx">pptx</a>, <a
                    href="files/presentations/13-evaluation.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#15 - Tue Oct 14</td>
            <td style="background-color: #ffe6e6;">
                Safety and alignment:
                <ul>
                    <li>Jailbreaking and adversarial attacks</li>
                    <li>Hallucination</li>
                    <li>Bias and fairness</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2307.15043.pdf">Universal and Transferable Adversarial Attacks on
                        Aligned Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2310.03693.pdf">Survey of Hallucination in Natural Language
                        Generation</a></li>
                </ol>
                [slides: <a href="files/presentations/14-safety.pptx">pptx</a>, <a
                    href="files/presentations/14-safety.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#16 - Thu Oct 16</td>
            <td style="background-color: #f5ffe6;">
                Efficiency:
                <ul>
                    <li>Model compression</li>
                    <li>Quantization</li>
                    <li>Efficient architectures</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ: Accurate Post-Training Quantization for
                        Generative Pre-trained Transformers</a></li>
                    <li><a href="https://arxiv.org/pdf/2401.18079.pdf">Mixtral of Experts</a></li>
                </ol>
                [slides: <a href="files/presentations/15-efficiency.pptx">pptx</a>, <a
                    href="files/presentations/15-efficiency.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#17 - Tue Oct 21</td>
            <td style="background-color: #e6e6ff;">
                Generalization and transfer:
                <ul>
                    <li>Zero-shot and few-shot generalization</li>
                    <li>Cross-lingual transfer</li>
                    <li>Domain adaptation</li>
                </ul>
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2210.03057.pdf">Cross-Lingual Generalization of Natural Language
                        Inference Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2312.12618.pdf">Domain Adaptation of Large Language Models</a>
                    </li>
                </ol>
                [slides: <a href="files/presentations/16-generalization.pptx">pptx</a>, <a
                    href="files/presentations/16-generalization.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#18 - Thu Oct 23</td>
            <td style="background-color: #ffe8c9;">
                Do we overestimate certain abilities of LLMs?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2408.16688v1">LLMs Do Not Think Step-By-Step In Implicit Reasoning</a></li>
                    <li><a href="https://arxiv.org/abs/2405.05229v1">Mirage: Model Internals Do Not Reflect What They Appear</a></li>
                </ol>
                [slides: <a href="files/presentations/ImpCoT-Oct23.pptx">pptx</a>, <a
                    href="files/presentations/ImpCoT-Oct23.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#19 - Tue Oct 28</td>
            <td style="background-color: #ffb8c9;">
                How do Reward Models and Preference Models work?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2410.14872">Rewarding Progress: Rethinking the Role of PPO in RLHF</a></li>
                    <li><a href="https://arxiv.org/pdf/2410.13639">Learning the Dynamics of the LLM Alignment</a></li>
                </ol>
                [slides: <a href="files/presentations/Reward-Oct28.pptx">pptx</a>, <a
                    href="files/presentations/Reward-Oct28.pdf">pdf</a>]
            </td>
        </tr>
        <tr class="sechighlight5">
            <td>Oct 28
            </td>
            <td>Progress report #1 deadline
            </td>
            <td></td>
        </tr>
        <tr>
            <td>#20 - Thu Oct 30</td>
            <td style="background-color: #ffc8e9;">
                How can LLMs effectively integrate retrieved information from multiple sources?
            </td>
            <td>
                <ol>
                    <li><a href="https://aclanthology.org/2024.naacl-long.337.pdf">Multi-Hop Table Retrieval for Open-Domain Text-to-SQL</a></li>
                    <li><a href="https://arxiv.org/pdf/2401.17393">DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text</a></li>
                </ol>
                [slides: <a href="files/presentations/Integration-Oct30.pptx">pptx</a>, <a
                    href="files/presentations/Integration-Oct30.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#21 - Tue Nov 4</td>
            <td style="background-color: #dfffe7;">
                How can we use LMs as inference engines?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2308.08299">Reasoning over Public and Private Data in Retrieval-Based Systems</a></li>
                    <li><a href="https://arxiv.org/pdf/2404.08161">TransformerFAM: Feedback attention is working memory</a></li>
                </ol>
                [slides: <a href="files/presentations/InfEngine-Nov4.pptx">pptx</a>, <a
                    href="files/presentations/InfEngine-Nov4.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#22 - Thu Nov 6</td>
            <td style="background-color: #fff0df;">
                How do we build LM with long-range coherence and consistency?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2407.01221">Loki: Low-Rank Keys for Efficient Sparse Attention</a></li>
                    <li><a href="https://arxiv.org/pdf/2407.02486">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</a></li>
                </ol>
                [slides: <a href="files/presentations/consistency-Nov6.pptx">pptx</a>, <a
                    href="files/presentations/consistency-Nov6.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#23 - Tue Nov 11</td>
            <td style="background-color: #ffe1f0;">
                Can we infer authors and texts that LM data was trained on?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2406.17711">DiscoveryBench: Towards Data-Driven Discovery with Large Language Models</a></li>
                    <li><a href="https://arxiv.org/pdf/2310.06632">Copyright Traps for Large Language Models</a></li>
                </ol>
                [slides: <a href="files/presentations/Infer-Nov11.pptx">pptx</a>, <a
                    href="files/presentations/Infer-Nov11.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#24 - Thu Nov 13</td>
            <td style="background-color: #e9dfff;">
                How can we distinguish AI-generated content from human content?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2410.08341">Watermark Anything with Localized Messages</a></li>
                    <li><a href="https://arxiv.org/pdf/2410.13823">Detecting Machine-Generated Long-Form Content with Latent-Space Variables</a></li>
                </ol>
                [slides: <a href="files/presentations/AI-content-Nov13.pptx">pptx</a>, <a
                    href="files/presentations/AI-content-Nov13.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#25 - Tue Nov 18</td>
            <td style="background-color: #f9dfff;">
                Can dynamic byte-level patches outperform fixed tokens as the basic unit for scaling LMs?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2412.09871">Byte Latent Transformer: Patches Scale Better  Than Tokens</a></li>
                    <li><a href="https://arxiv.org/pdf/2507.07955">Dynamic Chunking for End-to-End Hierarchical Sequence Modeling</a></li>
                </ol>
               [slides: <a href="files/presentations/HNETS-BLT-Nov18.pptx">pptx</a>, <a
                   href="files/presentations/HNETS-BLT-Nov18.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#26 - Thu Nov 20</td>
            <td style="background-color: #f4ffdf;">
                Do the "base" models already capable ot superior reasoning abilities seen in RL-ed or "thinking" LMs?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/abs/2510.14901">Reasoning with Sampling: Your Base Model is Smarter Than You Think</a></li>
                    <li><a href="https://arxiv.org/abs/2510.07364">Base Models Know How to Reason, Thinking Models Learn When</a></li>
                </ol>
               [slides: <a href="files/presentations/RL-Nov20.pptx">pptx</a>, <a
                   href="files/presentations/RL-Nov20.pdf">pdf</a>]
            </td>
        </tr>
        <tr class="sechighlight5">
            <td> Nov 21
            </td>
            <td>Progress report #2 deadline</td>
            <td></td>
        </tr>
        <tr class="sechighlight4 centered">
            <td>#27 - Tue Nov 25</td>
            <td>No Class - Fall Recess</td>
            <td></td>
        </tr>
        <tr class="sechighlight4 centered">
            <td>#28 - Thu Nov 27</td>
            <td>No Class - Fall Recess</td>
            <td></td>
        </tr>
        <tr>
            <td>#29 - Tue Dec 2</td>
            <td style="background-color: #fffddf;">
                Can internal representations of different LLMs be shared or mapped across models?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2510.03215">Cache-to-Cache: Direct Semantic Communication Between Large Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2506.00653">Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</a></li>
                </ol>
               [slides: <a href="files/presentations/Internal-rep-Dec2.pptx">pptx</a>, <a
                   href="files/presentations/Internal-rep-Dec2.pdf">pdf</a>]
            </td>
        </tr>
        <tr>
            <td>#30 - Thu Dec 4</td>
            <td style="background-color: #dfffe7;">
                Can sparse memory updates allow LLMs to continually learn?
            </td>
            <td>
                <ol>
                    <li><a href="https://arxiv.org/pdf/2412.09764">Memory Layers at Scale</a></li>
                    <li><a href="https://arxiv.org/pdf/2510.15103">Continual Learning via Sparse Memory Finetuning</a></li>
                </ol>
                [slides: <a href="files/presentations/Continual-learn-Dec4.pptx">pptx</a>, <a
                   href="files/presentations/Continual-learn-Dec4.pdf">pdf</a>]
            </td>
        </tr>

        <tr class="sechighlight4 centered">
            <td>Dec 8-11</td>
            <td>Reading Days</td>
            <td></td>
        </tr>
        <tr class="sechighlight5">
            <td>Dec 10</td>
            <td colspan="2">Final project poster session (2-4pm) --
                <a href="https://studentaffairs.jhu.edu/registrar/wp-content/uploads/sites/23/2025/08/Fall2025-_FinalExamSchedule.pdf">final
                exam schedule</a></td>
        </tr>
        <tr class="sechighlight5">
            <td> Dec 12
            </td>
            <td>Final project reports</td>
            <td>
            </td>
        </tr>
        </tbody>
    </table>
</div>
